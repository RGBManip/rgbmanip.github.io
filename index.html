<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RGBManip: Monocular Image-based Robotic Manipulation through Active Object Pose Estimation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <!-- <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div> -->
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">RGBManip: Monocular Image-based Robotic Manipulation through Active Object Pose Estimation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://keunhong.com">Boshi An*</a>,</span>
            <span class="author-block">
              <a href="https://utkarshsinha.com">Yiran Geng*</a>,</span>
            <span class="author-block">
              <a href="https://jonbarron.info">Kai Chen*</a>,
            </span>
            <span class="author-block">
              <a href="http://sofienbouaziz.com">Xiaoqi Li</a>,
            </span>
            <span class="author-block">
              <a href="https://www.danbgoldman.com">Qi Dou</a>,
            </span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~seitz/">Hao Dong</a>,
            </span>
          </div>

          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Washington,</span>
            <span class="author-block"><sup>2</sup>Google Research</span>
          </div> -->

        
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          Robotic manipulation requires accurate perception of the environment, which poses a significant challenge
due to its inherent complexity and constantly changing nature.
In this context, RGB image and point-cloud observations
are two commonly used modalities in visual-based robotic
manipulation, but each of these modalities have their own
limitations. Commercial point-cloud observations often suffer
from issues like sparse sampling and noisy output due to
the limits of the emission-reception imaging principle. On the
other hand, RGB images, while rich in texture information,
lack essential depth and 3D information crucial for robotic
manipulation. To mitigate these challenges, we propose an
image-only robotic manipulation framework that leverages
an eye-on-hand monocular camera installed on the robotâ€™s
parallel gripper. By moving with the robot gripper, this camera
gains the ability to actively perceive object from multiple
perspectives during the manipulation process. This enables
the estimation of 6D object poses, which can be utilized for
manipulation. While, obtaining images from more and diverse
viewpoints typically improves pose estimation, it also increases
the manipulation time. To address this trade-off, we employ a
reinforcement learning policy to synchronize the manipulation
strategy with active perception, achieving a balance between 6D
pose accuracy and manipulation efficiency. Our experimental
results in both simulated and real-world environments showcase
the state-of-the-art effectiveness of our approach. We believe
that our method will inspire further research on real-world-oriented robotic manipulation.
          
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>



<!-- 

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer> -->

</body>
</html>
